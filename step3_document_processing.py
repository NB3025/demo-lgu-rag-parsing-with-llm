"""
3ë‹¨ê³„: ë¬¸ì„œ ì²˜ë¦¬ ë° ì„ë² ë”© ìƒì„±
ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” PDF ë¬¸ì„œë¥¼ Claude 3.7 Sonnetìœ¼ë¡œ íŒŒì‹±í•˜ê³  Titan2 ì„ë² ë”©ì„ ìƒì„±í•˜ì—¬ OpenSearchì— ì¸ë±ì‹±í•©ë‹ˆë‹¤.
refer_parser_prompt.md í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™”ëœ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
"""

import json
import boto3
import time
import base64
from pathlib import Path
from typing import List, Dict, Any
import fitz  # PyMuPDF
from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth
import uuid
from datetime import datetime
import re

class DocumentProcessor:
    def __init__(self, config_file='opensearch_config.json'):
        """
        ë¬¸ì„œ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        
        Args:
            config_file (str): OpenSearch ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        # ì„¤ì • ë¡œë“œ
        self.config = self.load_config(config_file)
        if not self.config:
            raise Exception("ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # AWS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.region = self.config['region']
        self.bedrock_client = boto3.client('bedrock-runtime', region_name=self.region)
        self.s3_client = boto3.client('s3', region_name=self.region)
        
        # S3 ë²„í‚· ì´ë¦„ ìƒì„± (ê³ ìœ í•œ ì´ë¦„)
        account_id = boto3.client('sts', region_name=self.region).get_caller_identity()['Account']
        timestamp = str(int(time.time()))[-6:]
        self.bucket_name = f"rag-car-manual-{account_id}"
        
        # Claude API í˜¸ì¶œ ì œí•œ ê´€ë¦¬
        self.claude_call_count = 0  # í˜„ì¬ í˜¸ì¶œ íšŸìˆ˜
        self.claude_call_limit = 5  # 1ë¶„ë‹¹ ìµœëŒ€ í˜¸ì¶œ íšŸìˆ˜
        self.claude_window_start = time.time()  # í˜„ì¬ ìœˆë„ìš° ì‹œì‘ ì‹œê°„
        self.claude_window_duration = 60  # ìœˆë„ìš° ì§€ì† ì‹œê°„ (ì´ˆ)
        
        # OpenSearch í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        credentials = boto3.Session().get_credentials()
        self.awsauth = AWSV4SignerAuth(credentials, self.region, "aoss")
        self.oss_client = OpenSearch(
            hosts=[{"host": self.config['host'], "port": 443}],
            http_auth=self.awsauth,
            use_ssl=True,
            verify_certs=True,
            connection_class=RequestsHttpConnection,
            timeout=300,
        )
        
        # íŒŒì„œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
        self.parser_prompt = self.load_parser_prompt()
        
        print(f"ğŸ”— ë¬¸ì„œ ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"ğŸŒ ë¦¬ì „: {self.region}")
        print(f"ğŸª£ S3 ë²„í‚·: {self.bucket_name}")
        print(f"ğŸ”— OpenSearch ì—”ë“œí¬ì¸íŠ¸: {self.config['endpoint']}")
        print(f"ğŸ“„ ì¸ë±ìŠ¤: {self.config['index_name']}")
        print(f"ğŸ¤– LLM: Claude 3.7 Sonnet")
        print(f"ğŸ§  ì„ë² ë”©: {self.config['embedding_model']}")
    
    def create_s3_bucket(self):
        """
        S3 ë²„í‚· ìƒì„± (ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
        
        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ë²„í‚· ì¡´ì¬ í™•ì¸
            try:
                self.s3_client.head_bucket(Bucket=self.bucket_name)
                print(f"âœ… S3 ë²„í‚·ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {self.bucket_name}")
                return True
            except Exception as e:
                # 404 ë˜ëŠ” NoSuchBucket ì˜¤ë¥˜ëŠ” ë²„í‚·ì´ ì—†ë‹¤ëŠ” ì˜ë¯¸ì´ë¯€ë¡œ ìƒì„± ì§„í–‰
                if "404" in str(e) or "NoSuchBucket" in str(e) or "Not Found" in str(e):
                    print(f"ğŸ“ S3 ë²„í‚·ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤: {self.bucket_name}")
                else:
                    # ë‹¤ë¥¸ ì˜¤ë¥˜ëŠ” ì¬ë°œìƒ
                    raise e
            
            # ë²„í‚· ìƒì„±
            print(f"ğŸª£ S3 ë²„í‚· ìƒì„± ì¤‘: {self.bucket_name}")
            
            if self.region == 'us-east-1':
                # us-east-1ì€ LocationConstraintë¥¼ ì§€ì •í•˜ì§€ ì•ŠìŒ
                self.s3_client.create_bucket(Bucket=self.bucket_name)
            else:
                # ë‹¤ë¥¸ ë¦¬ì „ì€ LocationConstraint í•„ìš”
                self.s3_client.create_bucket(
                    Bucket=self.bucket_name,
                    CreateBucketConfiguration={'LocationConstraint': self.region}
                )
            
            print(f"âœ… S3 ë²„í‚· ìƒì„± ì™„ë£Œ: {self.bucket_name}")
            return True
            
        except Exception as e:
            print(f"âŒ S3 ë²„í‚· ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def upload_to_s3(self, file_path: str) -> str:
        """
        íŒŒì¼ì„ S3ì— ì—…ë¡œë“œ
        
        Args:
            file_path (str): ì—…ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ
            
        Returns:
            str: S3 URI
        """
        try:
            file_name = Path(file_path).name
            s3_key = f"documents/{file_name}"
            
            print(f"ğŸ“¤ S3ì— íŒŒì¼ ì—…ë¡œë“œ ì¤‘: {file_name}")
            
            # íŒŒì¼ ì—…ë¡œë“œ
            self.s3_client.upload_file(
                file_path, 
                self.bucket_name, 
                s3_key,
                ExtraArgs={'ContentType': 'application/pdf'}
            )
            
            # S3 URI ìƒì„±
            s3_uri = f"s3://{self.bucket_name}/{s3_key}"
            print(f"âœ… S3 ì—…ë¡œë“œ ì™„ë£Œ: {s3_uri}")
            
            return s3_uri
            
        except Exception as e:
            print(f"âŒ S3 ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
            return None
    
    def save_page_image_to_s3(self, image_base64: str, page_number: int, pdf_filename: str) -> str:
        """
        í˜ì´ì§€ ì´ë¯¸ì§€ë¥¼ S3ì— ì €ì¥
        
        Args:
            image_base64 (str): base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€
            page_number (int): í˜ì´ì§€ ë²ˆí˜¸ (0ë¶€í„° ì‹œì‘)
            pdf_filename (str): PDF íŒŒì¼ëª…
            
        Returns:
            str: ì´ë¯¸ì§€ S3 URI
        """
        try:
            # íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±°
            base_filename = Path(pdf_filename).stem
            image_key = f"images/{base_filename}_page_{page_number + 1}.png"
            
            # base64 ë””ì½”ë”©
            image_data = base64.b64decode(image_base64)
            
            # S3 ì—…ë¡œë“œ
            self.s3_client.put_object(
                Bucket=self.bucket_name,
                Key=image_key,
                Body=image_data,
                ContentType='image/png'
            )
            
            # S3 URI ìƒì„±
            image_s3_uri = f"s3://{self.bucket_name}/{image_key}"
            print(f"ğŸ–¼ï¸ í˜ì´ì§€ {page_number + 1} ì´ë¯¸ì§€ ì €ì¥: {image_s3_uri}")
            
            return image_s3_uri
            
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ S3 ì €ì¥ ì˜¤ë¥˜: {e}")
            return None
            s3_uri = f"s3://{self.bucket_name}/{s3_key}"
            print(f"âœ… S3 ì—…ë¡œë“œ ì™„ë£Œ: {s3_uri}")
            
            return s3_uri
            
        except Exception as e:
            print(f"âŒ S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            return ""
    
    def load_config(self, config_file):
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def load_parser_prompt(self, prompt_file='refer_parser_prompt.md'):
        """íŒŒì„œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ"""
        try:
            with open(prompt_file, 'r', encoding='utf-8') as f:
                prompt = f.read()
            print(f"ğŸ“„ íŒŒì„œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì™„ë£Œ: {prompt_file}")
            return prompt
        except Exception as e:
            print(f"âŒ íŒŒì„œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def pdf_to_images(self, pdf_path: str) -> List[Dict]:
        """
        PDFë¥¼ í˜ì´ì§€ë³„ ì´ë¯¸ì§€ë¡œ ë³€í™˜
        
        Args:
            pdf_path (str): PDF íŒŒì¼ ê²½ë¡œ
            
        Returns:
            List[Dict]: í˜ì´ì§€ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ“„ PDF íŒŒì¼ ë¡œë“œ ì¤‘: {pdf_path}")
        
        try:
            doc = fitz.open(pdf_path)
            pages = []
            
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                
                # í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ (300 DPI)
                mat = fitz.Matrix(300/72, 300/72)  # 300 DPI ë³€í™˜ ë§¤íŠ¸ë¦­ìŠ¤
                pix = page.get_pixmap(matrix=mat)
                img_data = pix.tobytes("png")
                
                # Base64 ì¸ì½”ë”©
                img_base64 = base64.b64encode(img_data).decode('utf-8')
                
                pages.append({
                    'page_number': page_num,
                    'image_base64': img_base64,
                    'image_format': 'png'
                })
                
                print(f"  ğŸ“„ í˜ì´ì§€ {page_num + 1}/{len(doc)} ë³€í™˜ ì™„ë£Œ")
            
            doc.close()
            print(f"âœ… PDF ë³€í™˜ ì™„ë£Œ: {len(pages)}í˜ì´ì§€")
            return pages
            
        except Exception as e:
            print(f"âŒ PDF ë³€í™˜ ì˜¤ë¥˜: {e}")
            return []
    
    def manage_claude_rate_limit(self):
        """
        Claude API í˜¸ì¶œ ì œí•œ ê´€ë¦¬
        1ë¶„ë‹¹ 5íšŒ ì œí•œì„ ì¤€ìˆ˜í•˜ê¸° ìœ„í•´ í•„ìš”ì‹œ ëŒ€ê¸°
        """
        current_time = time.time()
        
        # ìƒˆë¡œìš´ 1ë¶„ ìœˆë„ìš°ê°€ ì‹œì‘ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if current_time - self.claude_window_start >= self.claude_window_duration:
            # ìƒˆ ìœˆë„ìš° ì‹œì‘
            self.claude_window_start = current_time
            self.claude_call_count = 0
            print(f"ğŸ”„ ìƒˆë¡œìš´ API í˜¸ì¶œ ìœˆë„ìš° ì‹œì‘")
        
        # í˜¸ì¶œ ì œí•œì— ë„ë‹¬í–ˆëŠ”ì§€ í™•ì¸
        if self.claude_call_count >= self.claude_call_limit:
            # ë‹¤ìŒ ìœˆë„ìš°ê¹Œì§€ ëŒ€ê¸° ì‹œê°„ ê³„ì‚°
            elapsed = current_time - self.claude_window_start
            wait_time = self.claude_window_duration - elapsed
            
            if wait_time > 0:
                print(f"â³ API í˜¸ì¶œ ì œí•œ ë„ë‹¬ ({self.claude_call_count}/{self.claude_call_limit})")
                print(f"â° {wait_time:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
                
                # ì§„í–‰ë¥  í‘œì‹œí•˜ë©° ëŒ€ê¸°
                for remaining in range(int(wait_time), 0, -1):
                    print(f"   â±ï¸  {remaining}ì´ˆ ë‚¨ìŒ...", end='\r')
                    time.sleep(1)
                
                print(f"   âœ… ëŒ€ê¸° ì™„ë£Œ!                    ")
                
                # ìƒˆ ìœˆë„ìš° ì‹œì‘
                self.claude_window_start = time.time()
                self.claude_call_count = 0
        
        # í˜¸ì¶œ íšŸìˆ˜ ì¦ê°€
        self.claude_call_count += 1
        print(f"ğŸ¤– Claude API í˜¸ì¶œ ({self.claude_call_count}/{self.claude_call_limit})")
    
    def parse_page_with_claude(self, image_base64: str, page_number: int) -> str:
        """
        Claude 3.7 Sonnetìœ¼ë¡œ í˜ì´ì§€ íŒŒì‹± (API í˜¸ì¶œ ì œí•œ ì ìš©)
        
        Args:
            image_base64 (str): Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€
            page_number (int): í˜ì´ì§€ ë²ˆí˜¸
            
        Returns:
            str: íŒŒì‹±ëœ ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸
        """
        try:
            # API í˜¸ì¶œ ì œí•œ ê´€ë¦¬
            self.manage_claude_rate_limit()
            
            print(f"ğŸ¤– Claudeë¡œ í˜ì´ì§€ {page_number + 1} íŒŒì‹± ì¤‘...")
            
            # Claude 3.7 Sonnet í˜¸ì¶œ
            response = self.bedrock_client.invoke_model(
                modelId="us.anthropic.claude-3-7-sonnet-20250219-v1:0",
                body=json.dumps({
                    "anthropic_version": "bedrock-2023-05-31",
                    "max_tokens": 4000,
                    "messages": [
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": self.parser_prompt
                                },
                                {
                                    "type": "image",
                                    "source": {
                                        "type": "base64",
                                        "media_type": "image/png",
                                        "data": image_base64
                                    }
                                }
                            ]
                        }
                    ]
                })
            )
            
            # ì‘ë‹µ íŒŒì‹±
            result = json.loads(response['body'].read())
            print (f'{result=}')
            content = result['content'][0]['text']
            
            # <markdown></markdown> íƒœê·¸ì—ì„œ ë‚´ìš© ì¶”ì¶œ
            markdown_match = re.search(r'<markdown>(.*?)</markdown>', content, re.DOTALL)
            if markdown_match:
                parsed_content = markdown_match.group(1).strip()
                print(f"âœ… í˜ì´ì§€ {page_number + 1} íŒŒì‹± ì™„ë£Œ ({len(parsed_content)}ì)")
                return parsed_content
            else:
                print(f"âš ï¸ í˜ì´ì§€ {page_number + 1}: ë§ˆí¬ë‹¤ìš´ íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return content.strip()
                
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ {page_number + 1} íŒŒì‹± ì‹¤íŒ¨: {e}")
            return ""
    
    def chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° ê³ ë ¤)
        
        Args:
            text (str): ë¶„í• í•  í…ìŠ¤íŠ¸
            chunk_size (int): ì²­í¬ í¬ê¸°
            overlap (int): ê²¹ì¹˜ëŠ” ë¶€ë¶„ í¬ê¸°
            
        Returns:
            List[str]: ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        if not text or len(text) <= chunk_size:
            return [text] if text else []
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            # ì²­í¬ ëì´ í…ìŠ¤íŠ¸ ëì„ ë„˜ì§€ ì•Šë„ë¡ ì¡°ì •
            if end >= len(text):
                chunks.append(text[start:])
                break
            
            # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° ì‹œë„
            chunk_text = text[start:end]
            
            # ë§ˆí¬ë‹¤ìš´ í—¤ë”ë‚˜ ë¬¸ì¥ ëì—ì„œ ìë¥´ê¸°
            for delimiter in ['\n## ', '\n# ', '\n* ', '. ', '.\n', '\n']:
                last_delim = chunk_text.rfind(delimiter)
                if last_delim > chunk_size // 2:  # ì²­í¬ì˜ ì ˆë°˜ ì´ìƒì—ì„œ ë°œê²¬ëœ ê²½ìš°
                    end = start + last_delim + len(delimiter)
                    break
            
            chunks.append(text[start:end])
            start = end - overlap
        
        print(f"ğŸ“ í…ìŠ¤íŠ¸ ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
        return chunks
    
    def generate_embedding(self, text: str) -> List[float]:
        """
        Titan2ë¡œ ì„ë² ë”© ìƒì„±
        
        Args:
            text (str): ì„ë² ë”©í•  í…ìŠ¤íŠ¸
            
        Returns:
            List[float]: 1024ì°¨ì› ë²¡í„°
        """
        try:
            response = self.bedrock_client.invoke_model(
                modelId=self.config['embedding_model'],
                body=json.dumps({
                    "inputText": text,
                    "dimensions": self.config['vector_dimension'],
                    "normalize": True
                })
            )
            
            result = json.loads(response['body'].read())
            return result['embedding']
            
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    def index_document(self, chunks: List[str], source_uri: str, page_number: int, original_text: str, image_s3_uri: str = None):
        """
        ë¬¸ì„œ ì²­í¬ë¥¼ OpenSearchì— ì¸ë±ì‹±
        
        Args:
            chunks (List[str]): í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸
            source_uri (str): ì†ŒìŠ¤ URI
            page_number (int): í˜ì´ì§€ ë²ˆí˜¸
            original_text (str): ì›ë³¸ í…ìŠ¤íŠ¸
            image_s3_uri (str): í˜ì´ì§€ ì´ë¯¸ì§€ S3 URI (ì„ íƒì‚¬í•­)
        """
        print(f"ğŸ“¤ í˜ì´ì§€ {page_number + 1} ì¸ë±ì‹± ì¤‘... ({len(chunks)}ê°œ ì²­í¬)")
        
        for chunk_idx, chunk in enumerate(chunks):
            if not chunk.strip():
                continue
                
            try:
                # ì„ë² ë”© ìƒì„±
                embedding = self.generate_embedding(chunk)
                if not embedding:
                    print(f"âš ï¸ ì²­í¬ {chunk_idx + 1} ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                    continue
                
                # ë©”íƒ€ë°ì´í„° ìƒì„±
                metadata = {
                    "source": source_uri,
                    "parentText": original_text,
                    "relatedContent": []
                }
                
                # ì´ë¯¸ì§€ ì°¸ì¡° ì¶”ê°€
                if image_s3_uri:
                    metadata["relatedContent"].append({
                        "locationType": "S3",
                        "s3Location": {
                            "uri": image_s3_uri
                        }
                    })
                
                # ë¬¸ì„œ ìƒì„±
                doc = {
                    "id": str(uuid.uuid4()),
                    "AMAZON_BEDROCK_TEXT": chunk,
                    "AMAZON_BEDROCK_METADATA": json.dumps(metadata, ensure_ascii=False),
                    "bedrock-knowledge-base-default-vector": embedding,
                    "x-amz-bedrock-kb-source-uri": source_uri,
                    "x-amz-bedrock-kb-document-page-number": page_number,
                    "x-amz-bedrock-kb-data-source-id": "MANUAL_UPLOAD",
                    "title_extracted": self.extract_title(chunk),
                    "category": "ìë™ì°¨ë§¤ë‰´ì–¼",
                    "content_length": len(chunk),
                    "timestamp": datetime.now().isoformat()
                }
                
                # ì´ë¯¸ì§€ URIë¥¼ ë³„ë„ í•„ë“œë¡œë„ ì €ì¥ (ê²€ìƒ‰ í¸ì˜ì„±)
                if image_s3_uri:
                    doc["page_image_uri"] = image_s3_uri
                
                # OpenSearchì— ì¸ë±ì‹±
                response = self.oss_client.index(
                    index=self.config['index_name'],
                    body=doc
                )
                
                print(f"  âœ… ì²­í¬ {chunk_idx + 1}/{len(chunks)} ì¸ë±ì‹± ì™„ë£Œ")
                
                # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
                time.sleep(0.1)
                
            except Exception as e:
                print(f"âŒ ì²­í¬ {chunk_idx + 1} ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
    
    def extract_title(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ ì œëª© ì¶”ì¶œ"""
        lines = text.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('# '):
                return line[2:].strip()
            elif line.startswith('## '):
                return line[3:].strip()
        
        # ì²« ë²ˆì§¸ ë¬¸ì¥ì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš©
        first_sentence = text.split('.')[0].strip()
        return first_sentence[:100] if len(first_sentence) > 100 else first_sentence
    
    def process_document(self, pdf_path: str):
        """
        ì „ì²´ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        
        Args:
            pdf_path (str): PDF íŒŒì¼ ê²½ë¡œ
        """
        print(f"ğŸš€ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {pdf_path}")
        
        # 0. S3 ë²„í‚· ìƒì„± ë° íŒŒì¼ ì—…ë¡œë“œ
        if not self.create_s3_bucket():
            print("âŒ S3 ë²„í‚· ìƒì„± ì‹¤íŒ¨")
            return
        
        s3_uri = self.upload_to_s3(pdf_path)
        if not s3_uri:
            print("âŒ S3 ì—…ë¡œë“œ ì‹¤íŒ¨")
            return
        
        print(f"ğŸ“ ê¸°ë³¸ ì†ŒìŠ¤ URI: {s3_uri}")
        
        # 1. PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
        pages = self.pdf_to_images(pdf_path)
        if not pages:
            print("âŒ PDF ë³€í™˜ ì‹¤íŒ¨")
            return
        
        # 2. ê° í˜ì´ì§€ ì²˜ë¦¬ (API í˜¸ì¶œ ì œí•œ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 10í˜ì´ì§€ ì²˜ë¦¬)
        total_chunks = 0
        pdf_filename = Path(pdf_path).name
        max_pages = max(10, len(pages))  # ìµœëŒ€ 10í˜ì´ì§€ ë˜ëŠ” ì „ì²´ í˜ì´ì§€ ìˆ˜ ì¤‘ ì‘ì€ ê°’
        
        print(f"ğŸ“‹ ì´ {len(pages)}í˜ì´ì§€ ì¤‘ {max_pages}í˜ì´ì§€ ì²˜ë¦¬ ì˜ˆì •")
        print(f"âš ï¸  Claude API ì œí•œ: 1ë¶„ë‹¹ {self.claude_call_limit}íšŒ í˜¸ì¶œ")
        print(f"ğŸ’¡ 6ë²ˆì§¸ í˜¸ì¶œë¶€í„° 1ë¶„ ëŒ€ê¸°ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        
        for page_info in pages[:max_pages]:
            page_number = page_info['page_number']
            image_base64 = page_info['image_base64']
            
            print(f"\nğŸ“„ í˜ì´ì§€ {page_number + 1}/{len(pages)} ì²˜ë¦¬ ì¤‘...")
            
            # í˜ì´ì§€ ì´ë¯¸ì§€ë¥¼ S3ì— ì €ì¥
            image_s3_uri = self.save_page_image_to_s3(image_base64, page_number, pdf_filename)
            
            # Claudeë¡œ íŒŒì‹±
            parsed_text = self.parse_page_with_claude(image_base64, page_number)
            if not parsed_text:
                print(f"âš ï¸ í˜ì´ì§€ {page_number + 1} íŒŒì‹± ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
                continue
            
            # í…ìŠ¤íŠ¸ ì²­í‚¹
            chunks = self.chunk_text(parsed_text)
            if not chunks:
                print(f"âš ï¸ í˜ì´ì§€ {page_number + 1} ì²­í‚¹ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
                continue
            
            # í˜ì´ì§€ë³„ ê³ ìœ  URI ìƒì„± (S3 URI + í˜ì´ì§€ ë²ˆí˜¸)
            page_uri = f"{s3_uri}#page={page_number + 1}"
            print(f"ğŸ“ í˜ì´ì§€ {page_number + 1} URI: {page_uri}")
            
            # OpenSearchì— ì¸ë±ì‹± (í˜ì´ì§€ë³„ ê³ ìœ  URI ë° ì´ë¯¸ì§€ ì°¸ì¡° í¬í•¨)
            self.index_document(chunks, page_uri, page_number, parsed_text, image_s3_uri)
            
            total_chunks += len(chunks)
            
            print(f"âœ… í˜ì´ì§€ {page_number + 1} ì²˜ë¦¬ ì™„ë£Œ ({len(chunks)}ê°œ ì²­í¬)")
        
        print(f"\nğŸ‰ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ ì²˜ë¦¬ ê²°ê³¼:")
        print(f"   - í˜ì´ì§€ ìˆ˜: {len(pages)} (ì²˜ë¦¬: {max_pages}í˜ì´ì§€)")
        print(f"   - ì´ ì²­í¬ ìˆ˜: {total_chunks}")
        print(f"   - Claude API í˜¸ì¶œ: {self.claude_call_count}íšŒ")
        print(f"   - ê¸°ë³¸ S3 URI: {s3_uri}")
        print(f"   - í˜ì´ì§€ë³„ URI í˜•ì‹: {s3_uri}#page=N")
        print(f"   - ì¸ë±ìŠ¤: {self.config['index_name']}")
        
        # ì„¤ì • íŒŒì¼ì— S3 ì •ë³´ ì¶”ê°€
        self.config['s3_bucket'] = self.bucket_name
        self.config['s3_uri'] = s3_uri
        with open('opensearch_config.json', 'w', encoding='utf-8') as f:
            json.dump(self.config, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ S3 ì •ë³´ê°€ ì„¤ì • íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Step 3: ë¬¸ì„œ ì²˜ë¦¬ ë° ì„ë² ë”© ìƒì„±")
    print("ğŸ¤– LLM: Claude 3.7 Sonnet")
    print("ğŸ§  ì„ë² ë”©: Titan2")
    print("ğŸ“„ ë¬¸ì„œ: data/santafe.pdf")
    
    # ë¬¸ì„œ ê²½ë¡œ í™•ì¸
    pdf_path = "data/santafe.pdf"
    if not Path(pdf_path).exists():
        print(f"âŒ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        print("ğŸ’¡ data í´ë”ì— crob_santafe.pdf íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
        return
    
    try:
        # ë¬¸ì„œ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        processor = DocumentProcessor()
        
        # ë¬¸ì„œ ì²˜ë¦¬ ì‹¤í–‰
        processor.process_document(pdf_path)
        
        print("\nğŸ“ ë‹¤ìŒ ë‹¨ê³„:")
        print("1. step4_search_test.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ê²€ìƒ‰ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”")
        print("2. RAG ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ğŸ”§ í•´ê²° ë°©ë²•:")
        print("1. AWS ìê²© ì¦ëª…ì´ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
        print("2. Bedrock ëª¨ë¸ ê¶Œí•œì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
        print("3. OpenSearch Serverless ê¶Œí•œì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
        print("4. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:")
        print("   pip install PyMuPDF opensearch-py")

if __name__ == "__main__":
    main()
